{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of juliaGPU-playground.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "julia-1.0",
      "display_name": "Julia 1.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-OoYKNlqk1S",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 4\n",
        "Feel free to look around.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMSuTc3pDlHv",
        "colab_type": "code",
        "outputId": "5ef91365-0b5b-4052-ca3d-ecae7f324371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "using Pkg\n",
        "Pkg.add(\"CuArrays\")\n",
        "Pkg.add(\"CUDAnative\")\n",
        "Pkg.add(\"CUDAdrv\")\n",
        "using CUDAnative, CUDAdrv, CuArrays"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJjRELDjd-JB",
        "colab_type": "text"
      },
      "source": [
        "Also we shall add BenchmarkTools to benchmark our code and Test for testing code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L262sH1pXPBJ",
        "colab_type": "code",
        "outputId": "6fb3efb3-0748-44d3-8a9e-15e9fe124c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Pkg.add(\"BenchmarkTools\")\n",
        "Pkg.add(\"Test\")\n",
        "using BenchmarkTools, Test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Project.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n",
            "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.0/Manifest.toml`\n",
            "\u001b[90m [no changes]\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkpufapwzviF",
        "colab_type": "text"
      },
      "source": [
        "I wanted to define the elements globally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12-1bWy1z1_t",
        "colab_type": "code",
        "outputId": "d0b068e1-0b86-41dd-a96c-d039e19732f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "N=parse(Int64, readline())\n",
        "A=rand(Int32, (N,N))\n",
        "B=rand(Int32, (N,N))\n",
        "CPar=zeros(Int32, (N,N))\n",
        "CSeq=zeros(Int32, (N,N))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stdin> 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 Array{Int32,2}:\n",
              " 0  0\n",
              " 0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icoSew3siyyj",
        "colab_type": "text"
      },
      "source": [
        "legacy function for taking transpose, so I could supply the iteration to be in only one dimension. That way, I could multiply as\n",
        "\n",
        "@cuda threads and stuff C[i,j]=A[i, index]*B[i,index]\n",
        "\n",
        "where B is actually B transposed, thereby travesring with only one x. But then I found you could parallelize in blocks, and that was easier. This is just PoW."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPynUb9801_f",
        "colab_type": "code",
        "outputId": "a0ed239e-2002-4b63-dc04-d3cc4eb2cd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "B\n",
        "for i in 1:N\n",
        "  for j in 1:N\n",
        "    Bt[i,j]=B[j,i]\n",
        "   end\n",
        "end\n",
        "Bt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 Array{Int32,2}:\n",
              " 3  3\n",
              " 4  3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfIx-ZqiXBEg",
        "colab_type": "text"
      },
      "source": [
        "##Sequential multiplication\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8RLc63XE-s",
        "colab_type": "code",
        "outputId": "ae316386-cca4-4649-97ae-d18053f348a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function seq_mult!(A,B,N)\n",
        "    CSq=A*B\n",
        "    return CSq\n",
        "end"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "seq_mult! (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo6OFI00X1nK",
        "colab_type": "text"
      },
      "source": [
        "Now making the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMDUV5DtX3c7",
        "colab_type": "code",
        "outputId": "fc135ffa-8a6f-4ebf-8d99-bad182199112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "C=A*B\n",
        "CSeq=@btime seq_mult!(A,B,N)\n",
        "@test isapprox(C, CSeq)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  54.618 ns (1 allocation: 96 bytes)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrP7o6FGehC4",
        "colab_type": "text"
      },
      "source": [
        "##Parallel multiplication with one dimension of parallelisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17Z-X4be3sj",
        "colab_type": "text"
      },
      "source": [
        "The parallelisation in this bit is only in selecting the terms we multiply. We work in blocks, and multiply on and on. The multiplication itself is in parallel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9ybpijWjxVL",
        "colab_type": "text"
      },
      "source": [
        "Making the arrays cuda-ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1Eqcjvl44e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e3243f0e-625c-4b0d-ddd7-fdbe1d0c1fbf"
      },
      "source": [
        "d_a=CuArray(A)\n",
        "d_b=CuArray(B)\n",
        "d_c=CuArray(CPar)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 CuArray{Int32,2}:\n",
              " 0  0\n",
              " 0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufhojcp5ok6Z",
        "colab_type": "text"
      },
      "source": [
        "the final mult!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVCWROhpm-0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23622840-a42a-4ec4-cd1b-ef996631816a"
      },
      "source": [
        "function mult_par!(a,b,c,N)\n",
        "    tidx=threadIdx().x\n",
        "    tidy=threadIdx().y\n",
        "    bidx=blockIdx().x\n",
        "    bidy=blockIdx().y\n",
        "    ind_x= tidx + blockDim().x * (bidx-1)\n",
        "    ind_y= tidy + blockDim().y * (bidy-1)\n",
        "    if(ind_x <= N && ind_y <= N)\n",
        "        sum=0\n",
        "        for i=1:N\n",
        "            @inbounds sum=sum+a[ind_x,i]*b[i,ind_y]\n",
        "        end\n",
        "        @inbounds c[ind_x,ind_y]=sum\n",
        "    end\n",
        "    return\n",
        "end"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mult_par! (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybOu8tTcnCS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59c3289a-4728-47c6-e572-530024529020"
      },
      "source": [
        "@btime @cuda blocks=( Int(ceil(N/32)),Int(ceil(N/32)) ) threads=(32,32) mult_par!(d_a,d_b,d_c,N)\n",
        "CPar=Array(d_c);\n",
        "@test isapprox(C,CPar)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  9.853 μs (65 allocations: 2.48 KiB)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fghem6DnakIK",
        "colab_type": "text"
      },
      "source": [
        "##Parallel multiplication with dual parallelization(WIP)\n",
        "For this next bit, I've been working on it for some time now, so bear with me as I try to explain this:\n",
        "I knew we'd be solving for the the elements of the ith row and jth column, so I would parallelise that.\n",
        "Then I thought, why not parallelise the addition of the terms too? This would blow up the memory exponentially, but would theoretically make the execution quicker. That's where all my time went. But I don't know how to get this to work, something about not calling threads inside other threads. \n",
        "\n",
        "\n",
        "![alt text](https://i.imgflip.com/2vslvl.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMZ41Os-0a0f",
        "colab_type": "text"
      },
      "source": [
        "With the arrays defined, I needed to define function  par_mult, so that we could rowwise mult each element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514Fph-wBS50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_a = CuArray(A)\n",
        "d_b = CuArray(B)\n",
        "d_c = CuArray(C);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQ5qN66U-8Q",
        "colab_type": "code",
        "outputId": "c86a407c-ef04-48cf-ab8f-92873ca02ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "d_c"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2×2 CuArray{Int32,2}:\n",
              " 981038679  -1035465523\n",
              " 933358970    332883184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mt3OIZ8Rcmq",
        "colab_type": "text"
      },
      "source": [
        "Made a function to find the i-jth term, and defined it to run in parallel with the multiplication itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-EZim0j-3_R",
        "colab_type": "code",
        "outputId": "df1f4dae-995b-4f0f-a4f5-f42f37b05d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function find_term(d_a, d_b, x, y, term)\n",
        "    tidx=threadIdx().x\n",
        "    bidx=blockIdx().x\n",
        "    x_idx=blockDim().x*(bidx-1)+tidx\n",
        "    term+=d_a[x, x_idx]*d_b[x_idx, y]\n",
        "    \n",
        "end"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "find_term (generic function with 1 method)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icDpfJGI0kQe",
        "colab_type": "code",
        "outputId": "8ef60a2e-00de-4ea3-e905-90735f51c9c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "function par_mult!(d_a,d_b,d_c,N)\n",
        "    x = (blockIdx().x-1)*blockDim().x + threadIdx().x \n",
        "    y = (blockIdx().y-1)*blockDim().y + threadIdx().y \n",
        "    @cuda blocks=(Int(ceil(N/32)),Int(ceil(N/32)) ) threads=(32,32) find_term(d_a,d_b,x, y, d_c[x,y])\n",
        "end"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "par_mult! (generic function with 2 methods)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bnyMeVMdIbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1067
        },
        "outputId": "5ed8e3ff-db43-48f9-f8f6-1d528e4df427"
      },
      "source": [
        "@btime @cuda blocks=( Int(ceil(n/32)),Int(ceil(n/32)) ) threads=(32,32) par_mult!(d_a,d_b,d_c,N)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "┌ Info: Building the CUDAnative run-time library for your sm_70 device, this might take a while...\n",
            "└ @ CUDAnative /root/.julia/packages/CUDAnative/ytV2j/src/compiler/rtlib.jl:154\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "CUDAnative.InvalidIRError",
          "evalue": "ignored",
          "traceback": [
            "InvalidIRError: compiling par_mult!(CuDeviceArray{Int32,2,CUDAnative.AS.Global}, CuDeviceArray{Int32,2,CUDAnative.AS.Global}, CuDeviceArray{Int32,2,CUDAnative.AS.Global}, Int64) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to CUDAnative.#cufunction#159)\nStacktrace:\n [1] cufunction at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:348\n [2] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:174\n [3] macro expansion at gcutils.jl:87\n [4] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:171\n [5] par_mult! at In[29]:4\nReason: unsupported dynamic function invocation (call to #launch#24(blocks::Union{Integer, Tuple{Integer}, Tuple{Integer,Integer}, Tuple{Integer,Integer,Integer}}, threads::Union{Integer, Tuple{Integer}, Tuple{Integer,Integer}, Tuple{Integer,Integer,Integer}}, cooperative::Bool, shmem::Integer, stream::CuStream, ::Any, f::CuFunction, args...) in CUDAdrv at /root/.julia/packages/CUDAdrv/WVU1H/src/execution.jl:85)\nStacktrace:\n [1] #launch at none:0\n [2] #30 at /root/.julia/packages/CUDAdrv/WVU1H/src/execution.jl:179\n [3] macro expansion at gcutils.jl:87\n [4] macro expansion at /root/.julia/packages/CUDAdrv/WVU1H/src/execution.jl:139\n [5] convert_arguments at /root/.julia/packages/CUDAdrv/WVU1H/src/execution.jl:123\n [6] #cudacall#29 at /root/.julia/packages/CUDAdrv/WVU1H/src/execution.jl:178\n [7] #cudacall at none:0\n [8] #cudacall#158 at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:272\n [9] #cudacall at none:0\n [10] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:257\n [11] #call#146 at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:234\n [12] #call at none:0\n [13] #call#161 at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:398\n [14] HostKernel at none:0\n [15] macro expansion at gcutils.jl:87\n [16] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:171\n [17] par_mult! at In[29]:4",
            "",
            "Stacktrace:",
            " [1] check_ir(::CUDAnative.CompilerJob, ::LLVM.Module) at /root/.julia/packages/CUDAnative/ytV2j/src/compiler/validation.jl:114",
            " [2] macro expansion at /root/.julia/packages/TimerOutputs/7zSea/src/TimerOutput.jl:216 [inlined]",
            " [3] #codegen#119(::Bool, ::Bool, ::Bool, ::Bool, ::Bool, ::Function, ::Symbol, ::CUDAnative.CompilerJob) at /root/.julia/packages/CUDAnative/ytV2j/src/compiler/driver.jl:186",
            " [4] #codegen at /root/.julia/packages/CUDAnative/ytV2j/src/compiler/driver.jl:0 [inlined]",
            " [5] #compile#118(::Bool, ::Bool, ::Bool, ::Bool, ::Bool, ::Function, ::Symbol, ::CUDAnative.CompilerJob) at /root/.julia/packages/CUDAnative/ytV2j/src/compiler/driver.jl:47",
            " [6] #compile#117 at ./none:0 [inlined]",
            " [7] #compile at ./none:0 [inlined] (repeats 2 times)",
            " [8] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:380 [inlined]",
            " [9] #cufunction#159(::Nothing, ::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(cufunction), ::typeof(par_mult!), ::Type{Tuple{CuDeviceArray{Int32,2,CUDAnative.AS.Global},CuDeviceArray{Int32,2,CUDAnative.AS.Global},CuDeviceArray{Int32,2,CUDAnative.AS.Global},Int64}}) at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:348",
            " [10] cufunction(::Function, ::Type) at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:348",
            " [11] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:174 [inlined]",
            " [12] macro expansion at ./gcutils.jl:87 [inlined]",
            " [13] macro expansion at /root/.julia/packages/CUDAnative/ytV2j/src/execution.jl:171 [inlined]",
            " [14] ##core#485() at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:297",
            " [15] ##sample#486(::BenchmarkTools.Parameters) at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:303",
            " [16] #_run#14(::Bool, ::String, ::Base.Iterators.Pairs{Symbol,Integer,NTuple{4,Symbol},NamedTuple{(:samples, :evals, :gctrial, :gcsample),Tuple{Int64,Int64,Bool,Bool}}}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#484\")}, ::BenchmarkTools.Parameters) at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:331",
            " [17] (::getfield(Base, Symbol(\"#inner#2\")){Base.Iterators.Pairs{Symbol,Integer,NTuple{5,Symbol},NamedTuple{(:verbose, :samples, :evals, :gctrial, :gcsample),Tuple{Bool,Int64,Int64,Bool,Bool}}},typeof(BenchmarkTools._run),Tuple{BenchmarkTools.Benchmark{Symbol(\"##benchmark#484\")},BenchmarkTools.Parameters}})() at ./none:0",
            " [18] #invokelatest#1 at ./essentials.jl:701 [inlined]",
            " [19] #invokelatest at ./none:0 [inlined]",
            " [20] #run_result#19 at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:32 [inlined]",
            " [21] #run_result at ./none:0 [inlined]",
            " [22] #run#21(::Base.Iterators.Pairs{Symbol,Integer,NTuple{5,Symbol},NamedTuple{(:verbose, :samples, :evals, :gctrial, :gcsample),Tuple{Bool,Int64,Int64,Bool,Bool}}}, ::Function, ::BenchmarkTools.Benchmark{Symbol(\"##benchmark#484\")}, ::BenchmarkTools.Parameters) at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:46",
            " [23] #run at ./none:0 [inlined] (repeats 2 times)",
            " [24] #warmup#24 at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:79 [inlined]",
            " [25] warmup(::BenchmarkTools.Benchmark{Symbol(\"##benchmark#484\")}) at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:79",
            " [26] top-level scope at /root/.julia/packages/BenchmarkTools/mVOqg/src/execution.jl:390",
            " [27] top-level scope at In[30]:1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89bS4p64kPmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "function driverFunc()\n",
        "    for i in 1:N\n",
        "        @cuda threads=N runKernel(d_a, d_b, d_c)\n",
        "    end\n",
        "end"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}